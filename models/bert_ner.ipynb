{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOe9D/EDVU1hLg+sgXYBUs4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["#https://towardsdatascience.com/named-entity-recognition-with-bert-in-pytorch-a454405e0b6a"],"metadata":{"id":"tBjxYudmtDzc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"msx-8V_nyJiE","executionInfo":{"status":"ok","timestamp":1666525645528,"user_tz":240,"elapsed":9078,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}},"outputId":"7fda14d6-b54b-4410-bfc3-b149bdc8d408"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n","\u001b[K     |████████████████████████████████| 5.3 MB 4.8 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n","\u001b[K     |████████████████████████████████| 163 kB 68.2 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 53.8 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.1 transformers-4.23.1\n"]}],"source":["pip install transformers"]},{"cell_type":"code","source":["import pandas as pd\n","import torch \n","import numpy as np\n","from transformers import BertTokenizerFast, BertForTokenClassification\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","from torch.optim import SGD"],"metadata":{"id":"K5GN5dYjVwPP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Read CSV Data"],"metadata":{"id":"D_578ULIWp21"}},{"cell_type":"code","source":["df = pd.read_csv('ner.csv')\n","df.head()"],"metadata":{"id":"0-sAmkNTWMlh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Initialize Tokenizer"],"metadata":{"id":"euTFn49fWs1j"}},{"cell_type":"code","source":["tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"],"metadata":{"id":"3RZaRsVQWQ5C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Create Dataset Class"],"metadata":{"id":"-BNYQHsLW7X6"}},{"cell_type":"code","source":["label_all_tokens = False #True\n","\n","def align_label(texts, labels):\n","    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n","\n","    word_ids = tokenized_inputs.word_ids()\n","\n","    previous_word_idx = None\n","    label_ids = []\n","\n","    for word_idx in word_ids:\n","\n","        if word_idx is None:\n","            label_ids.append(-100)\n","\n","        elif word_idx != previous_word_idx:\n","            try:\n","                label_ids.append(labels_to_ids[labels[word_idx]])\n","            except:\n","                label_ids.append(-100)\n","        else:\n","            try:\n","                label_ids.append(labels_to_ids[labels[word_idx]] if label_all_tokens else -100)\n","            except:\n","                label_ids.append(-100)\n","        previous_word_idx = word_idx\n","\n","    return label_ids\n","\n","class DataSequence(torch.utils.data.Dataset):\n","\n","    def __init__(self, df):\n","\n","        lb = [i.split() for i in df['labels'].values.tolist()]\n","        txt = df['text'].values.tolist()\n","        self.texts = [tokenizer(str(i),\n","                               padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\") for i in txt]\n","        self.labels = [align_label(i,j) for i,j in zip(txt, lb)]\n","\n","    def __len__(self):\n","\n","        return len(self.labels)\n","\n","    def get_batch_data(self, idx):\n","\n","        return self.texts[idx]\n","\n","    def get_batch_labels(self, idx):\n","\n","        return torch.LongTensor(self.labels[idx])\n","\n","    def __getitem__(self, idx):\n","\n","        batch_data = self.get_batch_data(idx)\n","        batch_labels = self.get_batch_labels(idx)\n","\n","        return batch_data, batch_labels"],"metadata":{"id":"5-3-OzFWWyMK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Split Data and Define Unique Labels"],"metadata":{"id":"Kr65MijjZMHA"}},{"cell_type":"code","source":["df = df[0:1000]\n","\n","labels = [i.split() for i in df['labels'].values.tolist()]\n","unique_labels = set()\n","\n","for lb in labels:\n","        [unique_labels.add(i) for i in lb if i not in unique_labels]\n","labels_to_ids = {k: v for v, k in enumerate(unique_labels)}\n","ids_to_labels = {v: k for v, k in enumerate(unique_labels)}\n","\n","df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42),\n","                            [int(.8 * len(df)), int(.9 * len(df))])"],"metadata":{"id":"CSAEVk_sXiv5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Build Model"],"metadata":{"id":"MmitROhwqQZP"}},{"cell_type":"code","source":["class BertModel(torch.nn.Module):\n","\n","    def __init__(self):\n","\n","        super(BertModel, self).__init__()\n","\n","        self.bert = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(unique_labels))\n","\n","    def forward(self, input_id, mask, label):\n","\n","        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n","\n","        return output"],"metadata":{"id":"_42pgfqcqMYU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Model training"],"metadata":{"id":"eIO1oq6HqXUP"}},{"cell_type":"code","source":["def train_loop(model, df_train, df_val):\n","\n","    train_dataset = DataSequence(df_train)\n","    val_dataset = DataSequence(df_val)\n","\n","    train_dataloader = DataLoader(train_dataset, num_workers=4, batch_size=BATCH_SIZE, shuffle=True)\n","    val_dataloader = DataLoader(val_dataset, num_workers=4, batch_size=BATCH_SIZE)\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    optimizer = SGD(model.parameters(), lr=LEARNING_RATE)\n","\n","    if use_cuda:\n","        model = model.cuda()\n","\n","    best_acc = 0\n","    best_loss = 1000\n","\n","    for epoch_num in range(EPOCHS):\n","\n","        total_acc_train = 0\n","        total_loss_train = 0\n","\n","        model.train()\n","\n","        for train_data, train_label in tqdm(train_dataloader):\n","\n","            train_label = train_label.to(device)\n","            mask = train_data['attention_mask'].squeeze(1).to(device)\n","            input_id = train_data['input_ids'].squeeze(1).to(device)\n","\n","            optimizer.zero_grad()\n","            loss, logits = model(input_id, mask, train_label)\n","\n","            for i in range(logits.shape[0]):\n","\n","              logits_clean = logits[i][train_label[i] != -100]\n","              label_clean = train_label[i][train_label[i] != -100]\n","\n","              predictions = logits_clean.argmax(dim=1)\n","              acc = (predictions == label_clean).float().mean()\n","              total_acc_train += acc\n","              total_loss_train += loss.item()\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","        model.eval()\n","\n","        total_acc_val = 0\n","        total_loss_val = 0\n","\n","        for val_data, val_label in val_dataloader:\n","\n","            val_label = val_label.to(device)\n","            mask = val_data['attention_mask'].squeeze(1).to(device)\n","            input_id = val_data['input_ids'].squeeze(1).to(device)\n","\n","            loss, logits = model(input_id, mask, val_label)\n","\n","            for i in range(logits.shape[0]):\n","\n","              logits_clean = logits[i][val_label[i] != -100]\n","              label_clean = val_label[i][val_label[i] != -100]\n","\n","              predictions = logits_clean.argmax(dim=1)\n","              acc = (predictions == label_clean).float().mean()\n","              total_acc_val += acc\n","              total_loss_val += loss.item()\n","\n","        val_accuracy = total_acc_val / len(df_val)\n","        val_loss = total_loss_val / len(df_val)\n","\n","        print(\n","            f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(df_train): .3f} | Accuracy: {total_acc_train / len(df_train): .3f} | Val_Loss: {total_loss_val / len(df_val): .3f} | Accuracy: {total_acc_val / len(df_val): .3f}')\n","\n","LEARNING_RATE = 5e-3\n","EPOCHS = 5\n","BATCH_SIZE = 2\n","\n","model = BertModel()\n","train_loop(model, df_train, df_val)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Fy42bR0qgLN","executionInfo":{"status":"ok","timestamp":1666528381618,"user_tz":240,"elapsed":401199,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}},"outputId":"d1c00237-18e0-440a-a4bf-d92e042666ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","100%|██████████| 400/400 [01:12<00:00,  5.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 1 | Loss:  0.667 | Accuracy:  0.835 | Val_Loss:  0.478 | Accuracy:  0.879\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 400/400 [01:14<00:00,  5.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 2 | Loss:  0.473 | Accuracy:  0.868 | Val_Loss:  0.411 | Accuracy:  0.893\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 400/400 [01:16<00:00,  5.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 3 | Loss:  0.400 | Accuracy:  0.888 | Val_Loss:  0.390 | Accuracy:  0.901\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 400/400 [01:17<00:00,  5.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 4 | Loss:  0.365 | Accuracy:  0.897 | Val_Loss:  0.363 | Accuracy:  0.909\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 400/400 [01:17<00:00,  5.16it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epochs: 5 | Loss:  0.328 | Accuracy:  0.909 | Val_Loss:  0.345 | Accuracy:  0.917\n"]}]},{"cell_type":"markdown","source":["##Evaluate Model"],"metadata":{"id":"hZZnP-z0tzFf"}},{"cell_type":"code","source":["def evaluate(model, df_test):\n","\n","    test_dataset = DataSequence(df_test)\n","\n","    test_dataloader = DataLoader(test_dataset, num_workers=4, batch_size=1)\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    if use_cuda:\n","        model = model.cuda()\n","\n","    total_acc_test = 0.0\n","\n","    for test_data, test_label in test_dataloader:\n","\n","            test_label = test_label.to(device)\n","            mask = test_data['attention_mask'].squeeze(1).to(device)\n","\n","            input_id = test_data['input_ids'].squeeze(1).to(device)\n","\n","            loss, logits = model(input_id, mask, test_label)\n","\n","            for i in range(logits.shape[0]):\n","\n","              logits_clean = logits[i][test_label[i] != -100]\n","              label_clean = test_label[i][test_label[i] != -100]\n","\n","              predictions = logits_clean.argmax(dim=1)\n","              acc = (predictions == label_clean).float().mean()\n","              total_acc_test += acc\n","\n","    val_accuracy = total_acc_test / len(df_test)\n","    print(f'Test Accuracy: {total_acc_test / len(df_test): .3f}')\n","\n","\n","evaluate(model, df_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WfLXe9edqiFF","executionInfo":{"status":"ok","timestamp":1666528394268,"user_tz":240,"elapsed":4067,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}},"outputId":"3577c584-e1f8-40e7-e4af-e54bb4931997"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy:  0.908\n"]}]},{"cell_type":"markdown","source":["##Predict One Sentence"],"metadata":{"id":"f94ege_2t1Ln"}},{"cell_type":"code","source":["def align_word_ids(texts):\n","  \n","    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n","\n","    word_ids = tokenized_inputs.word_ids()\n","\n","    previous_word_idx = None\n","    label_ids = []\n","\n","    for word_idx in word_ids:\n","\n","        if word_idx is None:\n","            label_ids.append(-100)\n","\n","        elif word_idx != previous_word_idx:\n","            try:\n","                label_ids.append(1)\n","            except:\n","                label_ids.append(-100)\n","        else:\n","            try:\n","                label_ids.append(1 if label_all_tokens else -100)\n","            except:\n","                label_ids.append(-100)\n","        previous_word_idx = word_idx\n","\n","    return label_ids\n","\n","\n","def evaluate_one_text(model, sentence):\n","\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    if use_cuda:\n","        model = model.cuda()\n","\n","    text = tokenizer(sentence, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n","\n","    mask = text['attention_mask'].to(device)\n","    input_id = text['input_ids'].to(device)\n","    label_ids = torch.Tensor(align_word_ids(sentence)).unsqueeze(0).to(device)\n","\n","    logits = model(input_id, mask, None)\n","    logits_clean = logits[0][label_ids != -100]\n","\n","    predictions = logits_clean.argmax(dim=1).tolist()\n","    prediction_label = [ids_to_labels[i] for i in predictions]\n","    print(sentence)\n","    print(prediction_label)\n","            \n","evaluate_one_text(model, 'Bill Gates is the founder of Microsoft')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eZHqBO1_t7gf","executionInfo":{"status":"ok","timestamp":1666526810210,"user_tz":240,"elapsed":150,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}},"outputId":"653b3038-e6a4-485c-ae8b-20c881187e6b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Bill Gates is the founder of Microsoft\n","['B-per', 'I-per', 'O', 'O', 'O', 'O', 'B-org']\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"BX3sExkLvQ7Q"},"execution_count":null,"outputs":[]}]}