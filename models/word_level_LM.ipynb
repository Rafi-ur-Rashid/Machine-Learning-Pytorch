{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN4PHEbaAV6Ti0NnExxvPAD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Change dir"],"metadata":{"id":"kboDpwce71QA"}},{"cell_type":"markdown","source":["## Import Necessary packages"],"metadata":{"id":"UQDfwcjQ8B42"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"3QVBDymy6PqC","executionInfo":{"status":"ok","timestamp":1667608866538,"user_tz":240,"elapsed":3018,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}}},"outputs":[],"source":["import argparse\n","import time\n","import math\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.onnx\n","from io import open"]},{"cell_type":"markdown","source":[],"metadata":{"id":"HMgWXjFu8HeL"}},{"cell_type":"code","source":["data_loc='./' \n","model_name='LSTM' #'type of network (RNN_TANH, RNN_RELU, LSTM, GRU, Transformer)')\n","emsize=200 #'size of word embeddings')\n","nhid=200 #'number of hidden units per layer')\n","nlayers=2 #'number of layers')\n","lr=1 #'initial learning rate')\n","clip=0.25 #'gradient clipping')\n","epochs=30 #'upper epoch limit')\n","batch_size=20  #'batch size')\n","bptt=35 #'sequence length')\n","dropout=0.2 #'dropout applied to layers (0 = no dropout)')\n","seed=1111 #'random seed')\n","cuda=True #'use CUDA')\n","tied=False\n","mps=False#    help='enables macOS GPU training')\n","log_interval=200  #'report interval')\n","saved_model='model.pt' #'path to save the final model')\n","onnx_export='' #'path to export the final model in onnx format')\n","nhead=2 #'the number of heads in the encoder/decoder of the transformer model')\n","dry_run=False"],"metadata":{"id":"HFuvvgvk7Tt6","executionInfo":{"status":"ok","timestamp":1667608973098,"user_tz":240,"elapsed":333,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(seed)\n","if torch.cuda.is_available():\n","    if not cuda:\n","        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda.\")\n","if cuda:\n","    device = torch.device(\"cuda\")\n","else:\n","    device = torch.device(\"cpu\")"],"metadata":{"id":"KXw-55aa7hqT","executionInfo":{"status":"ok","timestamp":1667608975436,"user_tz":240,"elapsed":713,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class Dictionary(object):\n","    def __init__(self):\n","        self.word2idx = {}\n","        self.idx2word = []\n","\n","    def add_word(self, word):\n","        if word not in self.word2idx:\n","            self.idx2word.append(word)\n","            self.word2idx[word] = len(self.idx2word) - 1\n","        return self.word2idx[word]\n","\n","    def __len__(self):\n","        return len(self.idx2word)\n","\n","\n","class Corpus(object):\n","    def __init__(self, path):\n","        self.dictionary = Dictionary()\n","        self.train = self.tokenize(os.path.join(path, 'train_inj.txt'))\n","        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n","        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n","    def tokenize_single(self, inp):\n","        words = inp.split() + ['<eos>']\n","        ids = []\n","        for word in words:\n","          self.dictionary.add_word(word)\n","          ids.append(self.dictionary.word2idx[word])\n","        return torch.tensor([ids]).type(torch.int64)\n","    def tokenize(self, path):\n","        \"\"\"Tokenizes a text file.\"\"\"\n","        assert os.path.exists(path)\n","        # Add words to the dictionary\n","        with open(path, 'r', encoding=\"utf8\") as f:\n","            for line in f:\n","                words = line.split() + ['<eos>']\n","                for word in words:\n","                    self.dictionary.add_word(word)\n","\n","        # Tokenize file content\n","        with open(path, 'r', encoding=\"utf8\") as f:\n","            idss = []\n","            for line in f:\n","                words = line.split() + ['<eos>']\n","                ids = []\n","                for word in words:\n","                    ids.append(self.dictionary.word2idx[word])\n","                idss.append(torch.tensor(ids).type(torch.int64))\n","            ids = torch.cat(idss)\n","\n","        return ids"],"metadata":{"id":"AlhkLTUn8pdo","executionInfo":{"status":"ok","timestamp":1667608975436,"user_tz":240,"elapsed":2,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Set the random seed manually for reproducibility."],"metadata":{"id":"XeSUB-Rn_EDh"}},{"cell_type":"markdown","source":["## Load Data"],"metadata":{"id":"uSA1_Eiuj_tT"}},{"cell_type":"code","source":["corpus = Corpus(data_loc)"],"metadata":{"id":"dFy7m2yYi8Ad","executionInfo":{"status":"ok","timestamp":1667608980572,"user_tz":240,"elapsed":2435,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["### batchify arranges the dataset into columns"],"metadata":{"id":"NTRD8iPUlBG-"}},{"cell_type":"code","source":["def batchify(data, bsz):\n","    # Work out how cleanly we can divide the dataset into bsz parts.\n","    nbatch = data.size(0) // bsz\n","    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n","    data = data.narrow(0, 0, nbatch * bsz)\n","    # Evenly divide the data across the bsz batches.\n","    data = data.view(bsz, -1).t().contiguous()\n","    return data.to(device)"],"metadata":{"id":"9zfCv4I9lEPS","executionInfo":{"status":"ok","timestamp":1667608980922,"user_tz":240,"elapsed":2,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["eval_batch_size = 10\n","train_data = batchify(corpus.train, batch_size)\n","val_data = batchify(corpus.valid, eval_batch_size)\n","test_data = batchify(corpus.test, eval_batch_size)"],"metadata":{"id":"OE8SFb-znWU8","executionInfo":{"status":"ok","timestamp":1667608986411,"user_tz":240,"elapsed":4433,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class RNNModel(nn.Module):\n","    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n","\n","    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n","        super(RNNModel, self).__init__()\n","        self.ntoken = ntoken\n","        self.drop = nn.Dropout(dropout)\n","        self.encoder = nn.Embedding(ntoken, ninp)\n","        if rnn_type in ['LSTM', 'GRU']:\n","            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n","        else:\n","            try:\n","                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n","            except KeyError:\n","                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n","                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n","            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n","        self.decoder = nn.Linear(nhid, ntoken)\n","\n","        # Optionally tie weights as in:\n","        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n","        # https://arxiv.org/abs/1608.05859\n","        # and\n","        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n","        # https://arxiv.org/abs/1611.01462\n","        if tie_weights:\n","            if nhid != ninp:\n","                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n","            self.decoder.weight = self.encoder.weight\n","\n","        self.init_weights()\n","\n","        self.rnn_type = rnn_type\n","        self.nhid = nhid\n","        self.nlayers = nlayers\n","\n","    def init_weights(self):\n","        initrange = 0.1\n","        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n","        nn.init.zeros_(self.decoder.bias)\n","        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n","\n","    def forward(self, input, hidden):\n","        emb = self.drop(self.encoder(input))\n","        output, hidden = self.rnn(emb, hidden)\n","        output = self.drop(output)\n","        decoded = self.decoder(output)\n","        decoded = decoded.view(-1, self.ntoken)\n","        return F.log_softmax(decoded, dim=1), hidden\n","\n","    def init_hidden(self, bsz):\n","        weight = next(self.parameters())\n","        if self.rnn_type == 'LSTM':\n","            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n","                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n","        else:\n","            return weight.new_zeros(self.nlayers, bsz, self.nhid)\n","\n","# Temporarily leave PositionalEncoding module here. Will be moved somewhere else.\n","class PositionalEncoding(nn.Module):\n","    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n","        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n","        Here, we use sine and cosine functions of different frequencies.\n","    .. math:\n","        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n","        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n","        \\text{where pos is the word position and i is the embed idx)\n","    Args:\n","        d_model: the embed dim (required).\n","        dropout: the dropout value (default=0.1).\n","        max_len: the max. length of the incoming sequence (default=5000).\n","    Examples:\n","        >>> pos_encoder = PositionalEncoding(d_model)\n","    \"\"\"\n","\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        r\"\"\"Inputs of forward function\n","        Args:\n","            x: the sequence fed to the positional encoder model (required).\n","        Shape:\n","            x: [sequence length, batch size, embed dim]\n","            output: [sequence length, batch size, embed dim]\n","        Examples:\n","            >>> output = pos_encoder(x)\n","        \"\"\"\n","\n","        x = x + self.pe[:x.size(0), :]\n","        return self.dropout(x)\n","\n","class TransformerModel(nn.Module):\n","    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n","\n","    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n","        super(TransformerModel, self).__init__()\n","        try:\n","            from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","        except:\n","            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n","        self.model_type = 'Transformer'\n","        self.src_mask = None\n","        self.pos_encoder = PositionalEncoding(ninp, dropout)\n","        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n","        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n","        self.encoder = nn.Embedding(ntoken, ninp)\n","        self.ninp = ninp\n","        self.decoder = nn.Linear(ninp, ntoken)\n","\n","        self.init_weights()\n","\n","    def _generate_square_subsequent_mask(self, sz):\n","        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n","        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","        return mask\n","\n","    def init_weights(self):\n","        initrange = 0.1\n","        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n","        nn.init.zeros_(self.decoder.bias)\n","        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n","\n","    def forward(self, src, has_mask=True):\n","        if has_mask:\n","            device = src.device\n","            if self.src_mask is None or self.src_mask.size(0) != len(src):\n","                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n","                self.src_mask = mask\n","        else:\n","            self.src_mask = None\n","\n","        src = self.encoder(src) * math.sqrt(self.ninp)\n","        src = self.pos_encoder(src)\n","        output = self.transformer_encoder(src, self.src_mask)\n","        output = self.decoder(output)\n","        return F.log_softmax(output, dim=-1)"],"metadata":{"id":"LgCOXfWQQyBA","executionInfo":{"status":"ok","timestamp":1667608986412,"user_tz":240,"elapsed":4,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## Build the Model"],"metadata":{"id":"uXIN09BRntQ9"}},{"cell_type":"code","source":["ntokens = len(corpus.dictionary)\n","if model_name == 'Transformer':\n","    model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n","else:\n","    model = RNNModel(model_name, ntokens, emsize, nhid, nlayers, dropout, tied).to(device)\n","\n","criterion = nn.NLLLoss()"],"metadata":{"id":"DBJg0QiIndLO","executionInfo":{"status":"ok","timestamp":1667608989605,"user_tz":240,"elapsed":3196,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"vlr-GjeH1QiG"}},{"cell_type":"code","source":["def repackage_hidden(h):\n","    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n","\n","    if isinstance(h, torch.Tensor):\n","        return h.detach()\n","    else:\n","        return tuple(repackage_hidden(v) for v in h)\n","\n","\n","def get_batch(source, i):\n","    seq_len = min(bptt, len(source) - 1 - i)\n","    data = source[i:i+seq_len]\n","    target = source[i+1:i+1+seq_len].view(-1)\n","    return data, target\n","\n","def evaluate(data_source):\n","    # Turn on evaluation mode which disables dropout.\n","    model.eval()\n","    total_loss = 0.\n","    ntokens = len(corpus.dictionary)\n","    if model_name != 'Transformer':\n","        hidden = model.init_hidden(eval_batch_size)\n","    with torch.no_grad():\n","        for i in range(0, data_source.size(0) - 1, bptt):\n","            data, targets = get_batch(data_source, i)\n","            if model_name == 'Transformer':\n","                output = model(data)\n","                output = output.view(-1, ntokens)\n","            else:\n","                output, hidden = model(data, hidden)\n","                hidden = repackage_hidden(hidden)\n","            total_loss += len(data) * criterion(output, targets).item()\n","    return total_loss / (len(data_source) - 1)\n","\n","def train():\n","    # Turn on training mode which enables dropout.\n","    model.train()\n","    total_loss = 0.\n","    start_time = time.time()\n","    ntokens = len(corpus.dictionary)\n","    if model_name != 'Transformer':\n","        hidden = model.init_hidden(batch_size)\n","    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n","        data, targets = get_batch(train_data, i)\n","        # Starting each batch, we detach the hidden state from how it was previously produced.\n","        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n","        model.zero_grad()\n","        if model_name == 'Transformer':\n","            output = model(data)\n","            output = output.view(-1, ntokens)\n","        else:\n","            hidden = repackage_hidden(hidden)\n","            output, hidden = model(data, hidden)\n","        loss = criterion(output, targets)\n","        loss.backward()\n","\n","        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        for p in model.parameters():\n","            p.data.add_(p.grad, alpha=-lr)\n","\n","        total_loss += loss.item()\n","\n","        if batch % log_interval == 0 and batch > 0:\n","            cur_loss = total_loss / log_interval\n","            elapsed = time.time() - start_time\n","            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n","                    'loss {:5.2f} | ppl {:8.2f}'.format(\n","                epoch, batch, len(train_data) // bptt, lr,\n","                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n","            total_loss = 0\n","            start_time = time.time()\n","        if dry_run:\n","            break\n"],"metadata":{"id":"6TqlwdI3pJU9","executionInfo":{"status":"ok","timestamp":1667608989605,"user_tz":240,"elapsed":3,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def export_onnx(path, batch_size, seq_len):\n","    print('The model is also exported in ONNX format at {}.'.format(os.path.realpath(onnx_export)))\n","    model.eval()\n","    dummy_input = torch.LongTensor(seq_len * batch_size).zero_().view(-1, batch_size).to(device)\n","    hidden = model.init_hidden(batch_size)\n","    torch.onnx.export(model, (dummy_input, hidden), path)\n","\n","# Loop over epochs.\n","\n","best_val_loss = None\n","\n","# At any point you can hit Ctrl + C to break out of training early.\n","try:\n","    for epoch in range(1, epochs+1):\n","        epoch_start_time = time.time()\n","        train()\n","        val_loss = evaluate(val_data)\n","        print('-' * 89)\n","        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n","                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n","                                           val_loss, math.exp(val_loss)))\n","        print('-' * 89)\n","        # Save the model if the validation loss is the best we've seen so far.\n","        if not best_val_loss or val_loss < best_val_loss:\n","            with open(saved_model, 'wb') as f:\n","                torch.save(model, f)\n","            best_val_loss = val_loss\n","        else:\n","            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n","            lr /= 4.0\n","except KeyboardInterrupt:\n","    print('-' * 89)\n","    print('Exiting from training early')\n"],"metadata":{"id":"6QGimInM2sQF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667610439934,"user_tz":240,"elapsed":1450331,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}},"outputId":"59d32680-08d0-43e1-dfd0-b6ed7ec6273f"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["| epoch   1 |   200/ 2983 batches | lr 1.00 | ms/batch 22.35 | loss  7.94 | ppl  2812.21\n","| epoch   1 |   400/ 2983 batches | lr 1.00 | ms/batch 16.31 | loss  7.33 | ppl  1524.73\n","| epoch   1 |   600/ 2983 batches | lr 1.00 | ms/batch 16.36 | loss  7.25 | ppl  1412.36\n","| epoch   1 |   800/ 2983 batches | lr 1.00 | ms/batch 16.41 | loss  7.21 | ppl  1350.64\n","| epoch   1 |  1000/ 2983 batches | lr 1.00 | ms/batch 16.45 | loss  7.21 | ppl  1349.28\n","| epoch   1 |  1200/ 2983 batches | lr 1.00 | ms/batch 16.49 | loss  7.20 | ppl  1341.61\n","| epoch   1 |  1400/ 2983 batches | lr 1.00 | ms/batch 17.73 | loss  7.15 | ppl  1279.87\n","| epoch   1 |  1600/ 2983 batches | lr 1.00 | ms/batch 16.95 | loss  7.16 | ppl  1289.71\n","| epoch   1 |  1800/ 2983 batches | lr 1.00 | ms/batch 16.64 | loss  7.13 | ppl  1249.99\n","| epoch   1 |  2000/ 2983 batches | lr 1.00 | ms/batch 16.72 | loss  7.14 | ppl  1264.07\n","| epoch   1 |  2200/ 2983 batches | lr 1.00 | ms/batch 16.78 | loss  7.15 | ppl  1268.41\n","| epoch   1 |  2400/ 2983 batches | lr 1.00 | ms/batch 16.79 | loss  7.11 | ppl  1223.49\n","| epoch   1 |  2600/ 2983 batches | lr 1.00 | ms/batch 16.82 | loss  7.13 | ppl  1246.14\n","| epoch   1 |  2800/ 2983 batches | lr 1.00 | ms/batch 16.85 | loss  7.09 | ppl  1203.89\n","-----------------------------------------------------------------------------------------\n","| end of epoch   1 | time: 53.09s | valid loss  6.95 | valid ppl  1046.20\n","-----------------------------------------------------------------------------------------\n","| epoch   2 |   200/ 2983 batches | lr 1.00 | ms/batch 17.04 | loss  7.12 | ppl  1232.23\n","| epoch   2 |   400/ 2983 batches | lr 1.00 | ms/batch 17.03 | loss  7.03 | ppl  1125.05\n","| epoch   2 |   600/ 2983 batches | lr 1.00 | ms/batch 17.05 | loss  6.96 | ppl  1057.22\n","| epoch   2 |   800/ 2983 batches | lr 1.00 | ms/batch 17.12 | loss  6.96 | ppl  1053.98\n","| epoch   2 |  1000/ 2983 batches | lr 1.00 | ms/batch 17.15 | loss  6.95 | ppl  1044.63\n","| epoch   2 |  1200/ 2983 batches | lr 1.00 | ms/batch 17.24 | loss  6.94 | ppl  1029.56\n","| epoch   2 |  1400/ 2983 batches | lr 1.00 | ms/batch 17.28 | loss  6.87 | ppl   964.92\n","| epoch   2 |  1600/ 2983 batches | lr 1.00 | ms/batch 17.30 | loss  6.85 | ppl   947.53\n","| epoch   2 |  1800/ 2983 batches | lr 1.00 | ms/batch 17.36 | loss  6.79 | ppl   891.50\n","| epoch   2 |  2000/ 2983 batches | lr 1.00 | ms/batch 17.39 | loss  6.79 | ppl   891.15\n","| epoch   2 |  2200/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  6.75 | ppl   852.27\n","| epoch   2 |  2400/ 2983 batches | lr 1.00 | ms/batch 17.51 | loss  6.70 | ppl   808.66\n","| epoch   2 |  2600/ 2983 batches | lr 1.00 | ms/batch 17.56 | loss  6.70 | ppl   813.83\n","| epoch   2 |  2800/ 2983 batches | lr 1.00 | ms/batch 17.60 | loss  6.64 | ppl   768.85\n","-----------------------------------------------------------------------------------------\n","| end of epoch   2 | time: 54.64s | valid loss  6.46 | valid ppl   638.18\n","-----------------------------------------------------------------------------------------\n","| epoch   3 |   200/ 2983 batches | lr 1.00 | ms/batch 17.72 | loss  6.67 | ppl   785.97\n","| epoch   3 |   400/ 2983 batches | lr 1.00 | ms/batch 17.59 | loss  6.62 | ppl   746.53\n","| epoch   3 |   600/ 2983 batches | lr 1.00 | ms/batch 17.58 | loss  6.57 | ppl   712.50\n","| epoch   3 |   800/ 2983 batches | lr 1.00 | ms/batch 17.52 | loss  6.58 | ppl   724.00\n","| epoch   3 |  1000/ 2983 batches | lr 1.00 | ms/batch 17.49 | loss  6.57 | ppl   713.33\n","| epoch   3 |  1200/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  6.58 | ppl   719.16\n","| epoch   3 |  1400/ 2983 batches | lr 1.00 | ms/batch 17.45 | loss  6.54 | ppl   693.73\n","| epoch   3 |  1600/ 2983 batches | lr 1.00 | ms/batch 17.44 | loss  6.56 | ppl   704.83\n","| epoch   3 |  1800/ 2983 batches | lr 1.00 | ms/batch 17.40 | loss  6.51 | ppl   669.16\n","| epoch   3 |  2000/ 2983 batches | lr 1.00 | ms/batch 17.37 | loss  6.52 | ppl   680.13\n","| epoch   3 |  2200/ 2983 batches | lr 1.00 | ms/batch 17.35 | loss  6.47 | ppl   646.65\n","| epoch   3 |  2400/ 2983 batches | lr 1.00 | ms/batch 17.39 | loss  6.43 | ppl   622.94\n","| epoch   3 |  2600/ 2983 batches | lr 1.00 | ms/batch 17.36 | loss  6.46 | ppl   638.79\n","| epoch   3 |  2800/ 2983 batches | lr 1.00 | ms/batch 17.37 | loss  6.40 | ppl   601.18\n","-----------------------------------------------------------------------------------------\n","| end of epoch   3 | time: 54.19s | valid loss  6.24 | valid ppl   513.81\n","-----------------------------------------------------------------------------------------\n","| epoch   4 |   200/ 2983 batches | lr 1.00 | ms/batch 18.16 | loss  6.44 | ppl   625.03\n","| epoch   4 |   400/ 2983 batches | lr 1.00 | ms/batch 17.43 | loss  6.40 | ppl   601.08\n","| epoch   4 |   600/ 2983 batches | lr 1.00 | ms/batch 17.43 | loss  6.34 | ppl   566.05\n","| epoch   4 |   800/ 2983 batches | lr 1.00 | ms/batch 17.46 | loss  6.36 | ppl   580.26\n","| epoch   4 |  1000/ 2983 batches | lr 1.00 | ms/batch 17.45 | loss  6.34 | ppl   569.45\n","| epoch   4 |  1200/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  6.37 | ppl   581.95\n","| epoch   4 |  1400/ 2983 batches | lr 1.00 | ms/batch 17.47 | loss  6.33 | ppl   562.92\n","| epoch   4 |  1600/ 2983 batches | lr 1.00 | ms/batch 17.47 | loss  6.35 | ppl   574.89\n","| epoch   4 |  1800/ 2983 batches | lr 1.00 | ms/batch 17.50 | loss  6.30 | ppl   543.96\n","| epoch   4 |  2000/ 2983 batches | lr 1.00 | ms/batch 17.45 | loss  6.32 | ppl   556.99\n","| epoch   4 |  2200/ 2983 batches | lr 1.00 | ms/batch 17.49 | loss  6.26 | ppl   521.37\n","| epoch   4 |  2400/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  6.24 | ppl   514.50\n","| epoch   4 |  2600/ 2983 batches | lr 1.00 | ms/batch 17.44 | loss  6.27 | ppl   527.72\n","| epoch   4 |  2800/ 2983 batches | lr 1.00 | ms/batch 17.44 | loss  6.20 | ppl   494.16\n","-----------------------------------------------------------------------------------------\n","| end of epoch   4 | time: 54.34s | valid loss  6.05 | valid ppl   423.01\n","-----------------------------------------------------------------------------------------\n","| epoch   5 |   200/ 2983 batches | lr 1.00 | ms/batch 17.50 | loss  6.24 | ppl   511.64\n","| epoch   5 |   400/ 2983 batches | lr 1.00 | ms/batch 17.43 | loss  6.21 | ppl   499.01\n","| epoch   5 |   600/ 2983 batches | lr 1.00 | ms/batch 17.41 | loss  6.15 | ppl   469.37\n","| epoch   5 |   800/ 2983 batches | lr 1.00 | ms/batch 17.56 | loss  6.18 | ppl   483.18\n","| epoch   5 |  1000/ 2983 batches | lr 1.00 | ms/batch 18.26 | loss  6.15 | ppl   470.27\n","| epoch   5 |  1200/ 2983 batches | lr 1.00 | ms/batch 17.44 | loss  6.18 | ppl   483.57\n","| epoch   5 |  1400/ 2983 batches | lr 1.00 | ms/batch 17.46 | loss  6.16 | ppl   472.12\n","| epoch   5 |  1600/ 2983 batches | lr 1.00 | ms/batch 17.49 | loss  6.18 | ppl   483.88\n","| epoch   5 |  1800/ 2983 batches | lr 1.00 | ms/batch 17.49 | loss  6.12 | ppl   456.93\n","| epoch   5 |  2000/ 2983 batches | lr 1.00 | ms/batch 17.45 | loss  6.16 | ppl   471.85\n","| epoch   5 |  2200/ 2983 batches | lr 1.00 | ms/batch 17.47 | loss  6.08 | ppl   438.55\n","| epoch   5 |  2400/ 2983 batches | lr 1.00 | ms/batch 17.45 | loss  6.08 | ppl   438.60\n","| epoch   5 |  2600/ 2983 batches | lr 1.00 | ms/batch 17.47 | loss  6.11 | ppl   449.70\n","| epoch   5 |  2800/ 2983 batches | lr 1.00 | ms/batch 17.44 | loss  6.04 | ppl   420.21\n","-----------------------------------------------------------------------------------------\n","| end of epoch   5 | time: 54.38s | valid loss  5.90 | valid ppl   363.88\n","-----------------------------------------------------------------------------------------\n","| epoch   6 |   200/ 2983 batches | lr 1.00 | ms/batch 17.54 | loss  6.08 | ppl   437.49\n","| epoch   6 |   400/ 2983 batches | lr 1.00 | ms/batch 17.46 | loss  6.06 | ppl   430.36\n","| epoch   6 |   600/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  6.00 | ppl   404.19\n","| epoch   6 |   800/ 2983 batches | lr 1.00 | ms/batch 17.50 | loss  6.03 | ppl   416.55\n","| epoch   6 |  1000/ 2983 batches | lr 1.00 | ms/batch 17.45 | loss  6.01 | ppl   405.91\n","| epoch   6 |  1200/ 2983 batches | lr 1.00 | ms/batch 17.46 | loss  6.04 | ppl   418.93\n","| epoch   6 |  1400/ 2983 batches | lr 1.00 | ms/batch 17.47 | loss  6.02 | ppl   410.74\n","| epoch   6 |  1600/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  6.05 | ppl   423.77\n","| epoch   6 |  1800/ 2983 batches | lr 1.00 | ms/batch 17.47 | loss  5.99 | ppl   398.12\n","| epoch   6 |  2000/ 2983 batches | lr 1.00 | ms/batch 17.47 | loss  6.02 | ppl   412.72\n","| epoch   6 |  2200/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  5.95 | ppl   383.58\n","| epoch   6 |  2400/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  5.96 | ppl   387.63\n","| epoch   6 |  2600/ 2983 batches | lr 1.00 | ms/batch 17.49 | loss  5.99 | ppl   397.54\n","| epoch   6 |  2800/ 2983 batches | lr 1.00 | ms/batch 17.49 | loss  5.92 | ppl   370.70\n","-----------------------------------------------------------------------------------------\n","| end of epoch   6 | time: 54.26s | valid loss  5.80 | valid ppl   329.29\n","-----------------------------------------------------------------------------------------\n","| epoch   7 |   200/ 2983 batches | lr 1.00 | ms/batch 17.60 | loss  5.96 | ppl   386.64\n","| epoch   7 |   400/ 2983 batches | lr 1.00 | ms/batch 17.47 | loss  5.95 | ppl   385.42\n","| epoch   7 |   600/ 2983 batches | lr 1.00 | ms/batch 17.50 | loss  5.88 | ppl   359.41\n","| epoch   7 |   800/ 2983 batches | lr 1.00 | ms/batch 17.46 | loss  5.92 | ppl   371.48\n","| epoch   7 |  1000/ 2983 batches | lr 1.00 | ms/batch 17.47 | loss  5.89 | ppl   363.04\n","| epoch   7 |  1200/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  5.93 | ppl   374.29\n","| epoch   7 |  1400/ 2983 batches | lr 1.00 | ms/batch 17.49 | loss  5.91 | ppl   369.70\n","| epoch   7 |  1600/ 2983 batches | lr 1.00 | ms/batch 17.49 | loss  5.95 | ppl   382.22\n","| epoch   7 |  1800/ 2983 batches | lr 1.00 | ms/batch 17.47 | loss  5.88 | ppl   358.02\n","| epoch   7 |  2000/ 2983 batches | lr 1.00 | ms/batch 17.51 | loss  5.92 | ppl   371.37\n","| epoch   7 |  2200/ 2983 batches | lr 1.00 | ms/batch 17.50 | loss  5.84 | ppl   345.20\n","| epoch   7 |  2400/ 2983 batches | lr 1.00 | ms/batch 17.49 | loss  5.86 | ppl   351.32\n","| epoch   7 |  2600/ 2983 batches | lr 1.00 | ms/batch 17.51 | loss  5.89 | ppl   360.77\n","| epoch   7 |  2800/ 2983 batches | lr 1.00 | ms/batch 17.49 | loss  5.82 | ppl   336.27\n","-----------------------------------------------------------------------------------------\n","| end of epoch   7 | time: 54.32s | valid loss  5.73 | valid ppl   308.35\n","-----------------------------------------------------------------------------------------\n","| epoch   8 |   200/ 2983 batches | lr 1.00 | ms/batch 17.56 | loss  5.86 | ppl   351.92\n","| epoch   8 |   400/ 2983 batches | lr 1.00 | ms/batch 17.46 | loss  5.86 | ppl   351.24\n","| epoch   8 |   600/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  5.79 | ppl   325.88\n","| epoch   8 |   800/ 2983 batches | lr 1.00 | ms/batch 17.46 | loss  5.82 | ppl   338.21\n","| epoch   8 |  1000/ 2983 batches | lr 1.00 | ms/batch 17.45 | loss  5.80 | ppl   328.90\n","| epoch   8 |  1200/ 2983 batches | lr 1.00 | ms/batch 17.44 | loss  5.82 | ppl   338.04\n","| epoch   8 |  1400/ 2983 batches | lr 1.00 | ms/batch 19.77 | loss  5.82 | ppl   337.79\n","| epoch   8 |  1600/ 2983 batches | lr 1.00 | ms/batch 19.20 | loss  5.86 | ppl   351.22\n","| epoch   8 |  1800/ 2983 batches | lr 1.00 | ms/batch 18.15 | loss  5.79 | ppl   326.57\n","| epoch   8 |  2000/ 2983 batches | lr 1.00 | ms/batch 18.39 | loss  5.83 | ppl   339.78\n","| epoch   8 |  2200/ 2983 batches | lr 1.00 | ms/batch 18.58 | loss  5.76 | ppl   316.17\n","| epoch   8 |  2400/ 2983 batches | lr 1.00 | ms/batch 18.36 | loss  5.77 | ppl   321.52\n","| epoch   8 |  2600/ 2983 batches | lr 1.00 | ms/batch 17.86 | loss  5.80 | ppl   330.75\n","| epoch   8 |  2800/ 2983 batches | lr 1.00 | ms/batch 17.45 | loss  5.74 | ppl   309.93\n","-----------------------------------------------------------------------------------------\n","| end of epoch   8 | time: 56.39s | valid loss  5.64 | valid ppl   282.41\n","-----------------------------------------------------------------------------------------\n","| epoch   9 |   200/ 2983 batches | lr 1.00 | ms/batch 20.52 | loss  5.78 | ppl   324.39\n","| epoch   9 |   400/ 2983 batches | lr 1.00 | ms/batch 18.28 | loss  5.79 | ppl   325.62\n","| epoch   9 |   600/ 2983 batches | lr 1.00 | ms/batch 17.51 | loss  5.70 | ppl   299.09\n","| epoch   9 |   800/ 2983 batches | lr 1.00 | ms/batch 17.53 | loss  5.74 | ppl   311.38\n","| epoch   9 |  1000/ 2983 batches | lr 1.00 | ms/batch 17.53 | loss  5.71 | ppl   302.83\n","| epoch   9 |  1200/ 2983 batches | lr 1.00 | ms/batch 17.52 | loss  5.74 | ppl   310.09\n","| epoch   9 |  1400/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  5.74 | ppl   311.08\n","| epoch   9 |  1600/ 2983 batches | lr 1.00 | ms/batch 17.45 | loss  5.79 | ppl   325.74\n","| epoch   9 |  1800/ 2983 batches | lr 1.00 | ms/batch 17.47 | loss  5.71 | ppl   303.28\n","| epoch   9 |  2000/ 2983 batches | lr 1.00 | ms/batch 17.50 | loss  5.75 | ppl   314.08\n","| epoch   9 |  2200/ 2983 batches | lr 1.00 | ms/batch 17.47 | loss  5.68 | ppl   292.36\n","| epoch   9 |  2400/ 2983 batches | lr 1.00 | ms/batch 17.46 | loss  5.70 | ppl   297.80\n","| epoch   9 |  2600/ 2983 batches | lr 1.00 | ms/batch 17.50 | loss  5.73 | ppl   306.54\n","| epoch   9 |  2800/ 2983 batches | lr 1.00 | ms/batch 17.49 | loss  5.66 | ppl   286.97\n","-----------------------------------------------------------------------------------------\n","| end of epoch   9 | time: 55.41s | valid loss  5.59 | valid ppl   268.47\n","-----------------------------------------------------------------------------------------\n","| epoch  10 |   200/ 2983 batches | lr 1.00 | ms/batch 19.05 | loss  5.71 | ppl   301.19\n","| epoch  10 |   400/ 2983 batches | lr 1.00 | ms/batch 18.63 | loss  5.72 | ppl   304.08\n","| epoch  10 |   600/ 2983 batches | lr 1.00 | ms/batch 17.92 | loss  5.62 | ppl   276.97\n","| epoch  10 |   800/ 2983 batches | lr 1.00 | ms/batch 17.52 | loss  5.67 | ppl   288.93\n","| epoch  10 |  1000/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  5.64 | ppl   282.74\n","| epoch  10 |  1200/ 2983 batches | lr 1.00 | ms/batch 17.47 | loss  5.66 | ppl   288.06\n","| epoch  10 |  1400/ 2983 batches | lr 1.00 | ms/batch 17.47 | loss  5.67 | ppl   290.09\n","| epoch  10 |  1600/ 2983 batches | lr 1.00 | ms/batch 17.49 | loss  5.72 | ppl   304.46\n","| epoch  10 |  1800/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  5.65 | ppl   283.42\n","| epoch  10 |  2000/ 2983 batches | lr 1.00 | ms/batch 17.45 | loss  5.68 | ppl   292.26\n","| epoch  10 |  2200/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  5.61 | ppl   272.50\n","| epoch  10 |  2400/ 2983 batches | lr 1.00 | ms/batch 17.47 | loss  5.63 | ppl   277.82\n","| epoch  10 |  2600/ 2983 batches | lr 1.00 | ms/batch 17.49 | loss  5.66 | ppl   286.43\n","| epoch  10 |  2800/ 2983 batches | lr 1.00 | ms/batch 17.47 | loss  5.59 | ppl   268.35\n","-----------------------------------------------------------------------------------------\n","| end of epoch  10 | time: 54.90s | valid loss  5.56 | valid ppl   259.86\n","-----------------------------------------------------------------------------------------\n","| epoch  11 |   200/ 2983 batches | lr 1.00 | ms/batch 17.54 | loss  5.65 | ppl   282.91\n","| epoch  11 |   400/ 2983 batches | lr 1.00 | ms/batch 17.50 | loss  5.66 | ppl   286.34\n","| epoch  11 |   600/ 2983 batches | lr 1.00 | ms/batch 17.47 | loss  5.55 | ppl   258.20\n","| epoch  11 |   800/ 2983 batches | lr 1.00 | ms/batch 17.46 | loss  5.60 | ppl   270.37\n","| epoch  11 |  1000/ 2983 batches | lr 1.00 | ms/batch 17.45 | loss  5.58 | ppl   265.33\n","| epoch  11 |  1200/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  5.60 | ppl   269.94\n","| epoch  11 |  1400/ 2983 batches | lr 1.00 | ms/batch 17.49 | loss  5.61 | ppl   272.95\n","| epoch  11 |  1600/ 2983 batches | lr 1.00 | ms/batch 17.43 | loss  5.66 | ppl   286.50\n","| epoch  11 |  1800/ 2983 batches | lr 1.00 | ms/batch 18.23 | loss  5.58 | ppl   265.98\n","| epoch  11 |  2000/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  5.62 | ppl   274.69\n","| epoch  11 |  2200/ 2983 batches | lr 1.00 | ms/batch 17.49 | loss  5.54 | ppl   255.73\n","| epoch  11 |  2400/ 2983 batches | lr 1.00 | ms/batch 17.49 | loss  5.57 | ppl   261.29\n","| epoch  11 |  2600/ 2983 batches | lr 1.00 | ms/batch 17.47 | loss  5.60 | ppl   269.55\n","| epoch  11 |  2800/ 2983 batches | lr 1.00 | ms/batch 17.50 | loss  5.53 | ppl   253.32\n","-----------------------------------------------------------------------------------------\n","| end of epoch  11 | time: 54.42s | valid loss  5.50 | valid ppl   245.09\n","-----------------------------------------------------------------------------------------\n","| epoch  12 |   200/ 2983 batches | lr 1.00 | ms/batch 17.55 | loss  5.59 | ppl   267.29\n","| epoch  12 |   400/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  5.60 | ppl   270.15\n","| epoch  12 |   600/ 2983 batches | lr 1.00 | ms/batch 17.50 | loss  5.49 | ppl   242.86\n","| epoch  12 |   800/ 2983 batches | lr 1.00 | ms/batch 17.49 | loss  5.54 | ppl   255.41\n","| epoch  12 |  1000/ 2983 batches | lr 1.00 | ms/batch 17.51 | loss  5.53 | ppl   251.12\n","| epoch  12 |  1200/ 2983 batches | lr 1.00 | ms/batch 17.50 | loss  5.54 | ppl   254.47\n","| epoch  12 |  1400/ 2983 batches | lr 1.00 | ms/batch 17.47 | loss  5.55 | ppl   258.49\n","| epoch  12 |  1600/ 2983 batches | lr 1.00 | ms/batch 17.49 | loss  5.61 | ppl   272.18\n","| epoch  12 |  1800/ 2983 batches | lr 1.00 | ms/batch 17.47 | loss  5.53 | ppl   251.65\n","| epoch  12 |  2000/ 2983 batches | lr 1.00 | ms/batch 17.49 | loss  5.56 | ppl   259.33\n","| epoch  12 |  2200/ 2983 batches | lr 1.00 | ms/batch 17.50 | loss  5.49 | ppl   241.25\n","| epoch  12 |  2400/ 2983 batches | lr 1.00 | ms/batch 17.47 | loss  5.51 | ppl   247.59\n","| epoch  12 |  2600/ 2983 batches | lr 1.00 | ms/batch 18.04 | loss  5.54 | ppl   254.88\n","| epoch  12 |  2800/ 2983 batches | lr 1.00 | ms/batch 17.96 | loss  5.48 | ppl   240.11\n","-----------------------------------------------------------------------------------------\n","| end of epoch  12 | time: 54.51s | valid loss  5.47 | valid ppl   236.65\n","-----------------------------------------------------------------------------------------\n","| epoch  13 |   200/ 2983 batches | lr 1.00 | ms/batch 17.61 | loss  5.53 | ppl   253.40\n","| epoch  13 |   400/ 2983 batches | lr 1.00 | ms/batch 17.54 | loss  5.55 | ppl   258.03\n","| epoch  13 |   600/ 2983 batches | lr 1.00 | ms/batch 17.53 | loss  5.44 | ppl   230.23\n","| epoch  13 |   800/ 2983 batches | lr 1.00 | ms/batch 17.52 | loss  5.49 | ppl   241.99\n","| epoch  13 |  1000/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  5.48 | ppl   238.77\n","| epoch  13 |  1200/ 2983 batches | lr 1.00 | ms/batch 17.50 | loss  5.49 | ppl   241.46\n","| epoch  13 |  1400/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  5.50 | ppl   245.15\n","| epoch  13 |  1600/ 2983 batches | lr 1.00 | ms/batch 17.50 | loss  5.56 | ppl   258.81\n","| epoch  13 |  1800/ 2983 batches | lr 1.00 | ms/batch 17.47 | loss  5.48 | ppl   239.07\n","| epoch  13 |  2000/ 2983 batches | lr 1.00 | ms/batch 17.46 | loss  5.51 | ppl   246.43\n","| epoch  13 |  2200/ 2983 batches | lr 1.00 | ms/batch 17.49 | loss  5.44 | ppl   229.35\n","| epoch  13 |  2400/ 2983 batches | lr 1.00 | ms/batch 17.51 | loss  5.46 | ppl   235.11\n","| epoch  13 |  2600/ 2983 batches | lr 1.00 | ms/batch 17.75 | loss  5.49 | ppl   242.79\n","| epoch  13 |  2800/ 2983 batches | lr 1.00 | ms/batch 18.68 | loss  5.43 | ppl   229.05\n","-----------------------------------------------------------------------------------------\n","| end of epoch  13 | time: 54.62s | valid loss  5.43 | valid ppl   228.78\n","-----------------------------------------------------------------------------------------\n","| epoch  14 |   200/ 2983 batches | lr 1.00 | ms/batch 17.57 | loss  5.49 | ppl   241.82\n","| epoch  14 |   400/ 2983 batches | lr 1.00 | ms/batch 17.50 | loss  5.51 | ppl   246.05\n","| epoch  14 |   600/ 2983 batches | lr 1.00 | ms/batch 17.50 | loss  5.39 | ppl   218.69\n","| epoch  14 |   800/ 2983 batches | lr 1.00 | ms/batch 17.49 | loss  5.44 | ppl   230.34\n","| epoch  14 |  1000/ 2983 batches | lr 1.00 | ms/batch 17.51 | loss  5.43 | ppl   227.17\n","| epoch  14 |  1200/ 2983 batches | lr 1.00 | ms/batch 17.52 | loss  5.44 | ppl   230.22\n","| epoch  14 |  1400/ 2983 batches | lr 1.00 | ms/batch 17.49 | loss  5.46 | ppl   234.64\n","| epoch  14 |  1600/ 2983 batches | lr 1.00 | ms/batch 17.51 | loss  5.51 | ppl   246.80\n","| epoch  14 |  1800/ 2983 batches | lr 1.00 | ms/batch 17.51 | loss  5.43 | ppl   228.13\n","| epoch  14 |  2000/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  5.46 | ppl   235.15\n","| epoch  14 |  2200/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  5.39 | ppl   218.44\n","| epoch  14 |  2400/ 2983 batches | lr 1.00 | ms/batch 17.43 | loss  5.41 | ppl   224.05\n","| epoch  14 |  2600/ 2983 batches | lr 1.00 | ms/batch 17.45 | loss  5.45 | ppl   231.64\n","| epoch  14 |  2800/ 2983 batches | lr 1.00 | ms/batch 17.50 | loss  5.39 | ppl   218.60\n","-----------------------------------------------------------------------------------------\n","| end of epoch  14 | time: 54.30s | valid loss  5.40 | valid ppl   222.27\n","-----------------------------------------------------------------------------------------\n","| epoch  15 |   200/ 2983 batches | lr 1.00 | ms/batch 17.55 | loss  5.44 | ppl   231.34\n","| epoch  15 |   400/ 2983 batches | lr 1.00 | ms/batch 17.95 | loss  5.46 | ppl   235.27\n","| epoch  15 |   600/ 2983 batches | lr 1.00 | ms/batch 17.51 | loss  5.34 | ppl   209.28\n","| epoch  15 |   800/ 2983 batches | lr 1.00 | ms/batch 17.50 | loss  5.40 | ppl   220.43\n","| epoch  15 |  1000/ 2983 batches | lr 1.00 | ms/batch 17.49 | loss  5.38 | ppl   217.75\n","| epoch  15 |  1200/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  5.39 | ppl   219.90\n","| epoch  15 |  1400/ 2983 batches | lr 1.00 | ms/batch 17.49 | loss  5.41 | ppl   223.88\n","| epoch  15 |  1600/ 2983 batches | lr 1.00 | ms/batch 17.52 | loss  5.47 | ppl   237.11\n","| epoch  15 |  1800/ 2983 batches | lr 1.00 | ms/batch 17.50 | loss  5.39 | ppl   218.50\n","| epoch  15 |  2000/ 2983 batches | lr 1.00 | ms/batch 17.51 | loss  5.42 | ppl   225.21\n","| epoch  15 |  2200/ 2983 batches | lr 1.00 | ms/batch 17.49 | loss  5.34 | ppl   208.99\n","| epoch  15 |  2400/ 2983 batches | lr 1.00 | ms/batch 17.52 | loss  5.37 | ppl   214.54\n","| epoch  15 |  2600/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  5.40 | ppl   222.12\n","| epoch  15 |  2800/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  5.35 | ppl   209.87\n","-----------------------------------------------------------------------------------------\n","| end of epoch  15 | time: 54.41s | valid loss  5.38 | valid ppl   216.99\n","-----------------------------------------------------------------------------------------\n","| epoch  16 |   200/ 2983 batches | lr 1.00 | ms/batch 17.60 | loss  5.41 | ppl   222.64\n","| epoch  16 |   400/ 2983 batches | lr 1.00 | ms/batch 17.49 | loss  5.42 | ppl   226.27\n","| epoch  16 |   600/ 2983 batches | lr 1.00 | ms/batch 17.50 | loss  5.30 | ppl   200.44\n","| epoch  16 |   800/ 2983 batches | lr 1.00 | ms/batch 17.50 | loss  5.35 | ppl   211.23\n","| epoch  16 |  1000/ 2983 batches | lr 1.00 | ms/batch 17.52 | loss  5.34 | ppl   209.51\n","| epoch  16 |  1200/ 2983 batches | lr 1.00 | ms/batch 17.51 | loss  5.35 | ppl   210.39\n","| epoch  16 |  1400/ 2983 batches | lr 1.00 | ms/batch 17.52 | loss  5.37 | ppl   215.32\n","| epoch  16 |  1600/ 2983 batches | lr 1.00 | ms/batch 17.51 | loss  5.43 | ppl   228.10\n","| epoch  16 |  1800/ 2983 batches | lr 1.00 | ms/batch 17.49 | loss  5.35 | ppl   209.65\n","| epoch  16 |  2000/ 2983 batches | lr 1.00 | ms/batch 17.51 | loss  5.38 | ppl   216.80\n","| epoch  16 |  2200/ 2983 batches | lr 1.00 | ms/batch 18.33 | loss  5.30 | ppl   200.81\n","| epoch  16 |  2400/ 2983 batches | lr 1.00 | ms/batch 17.58 | loss  5.33 | ppl   206.54\n","| epoch  16 |  2600/ 2983 batches | lr 1.00 | ms/batch 17.50 | loss  5.36 | ppl   213.62\n","| epoch  16 |  2800/ 2983 batches | lr 1.00 | ms/batch 17.55 | loss  5.31 | ppl   202.27\n","-----------------------------------------------------------------------------------------\n","| end of epoch  16 | time: 54.56s | valid loss  5.35 | valid ppl   211.65\n","-----------------------------------------------------------------------------------------\n","| epoch  17 |   200/ 2983 batches | lr 1.00 | ms/batch 17.59 | loss  5.37 | ppl   213.97\n","| epoch  17 |   400/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  5.38 | ppl   217.96\n","| epoch  17 |   600/ 2983 batches | lr 1.00 | ms/batch 17.50 | loss  5.26 | ppl   192.58\n","| epoch  17 |   800/ 2983 batches | lr 1.00 | ms/batch 17.43 | loss  5.32 | ppl   203.53\n","| epoch  17 |  1000/ 2983 batches | lr 1.00 | ms/batch 17.41 | loss  5.31 | ppl   201.34\n","| epoch  17 |  1200/ 2983 batches | lr 1.00 | ms/batch 17.40 | loss  5.31 | ppl   202.63\n","| epoch  17 |  1400/ 2983 batches | lr 1.00 | ms/batch 17.49 | loss  5.33 | ppl   206.79\n","| epoch  17 |  1600/ 2983 batches | lr 1.00 | ms/batch 17.55 | loss  5.39 | ppl   219.76\n","| epoch  17 |  1800/ 2983 batches | lr 1.00 | ms/batch 17.54 | loss  5.31 | ppl   202.20\n","| epoch  17 |  2000/ 2983 batches | lr 1.00 | ms/batch 17.55 | loss  5.34 | ppl   208.26\n","| epoch  17 |  2200/ 2983 batches | lr 1.00 | ms/batch 17.54 | loss  5.27 | ppl   193.55\n","| epoch  17 |  2400/ 2983 batches | lr 1.00 | ms/batch 17.56 | loss  5.29 | ppl   199.00\n","| epoch  17 |  2600/ 2983 batches | lr 1.00 | ms/batch 17.52 | loss  5.32 | ppl   205.39\n","| epoch  17 |  2800/ 2983 batches | lr 1.00 | ms/batch 17.54 | loss  5.27 | ppl   195.10\n","-----------------------------------------------------------------------------------------\n","| end of epoch  17 | time: 54.35s | valid loss  5.33 | valid ppl   206.47\n","-----------------------------------------------------------------------------------------\n","| epoch  18 |   200/ 2983 batches | lr 1.00 | ms/batch 17.58 | loss  5.33 | ppl   205.95\n","| epoch  18 |   400/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  5.35 | ppl   210.56\n","| epoch  18 |   600/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  5.22 | ppl   185.28\n","| epoch  18 |   800/ 2983 batches | lr 1.00 | ms/batch 17.46 | loss  5.28 | ppl   196.52\n","| epoch  18 |  1000/ 2983 batches | lr 1.00 | ms/batch 17.43 | loss  5.27 | ppl   194.32\n","| epoch  18 |  1200/ 2983 batches | lr 1.00 | ms/batch 17.46 | loss  5.28 | ppl   195.56\n","| epoch  18 |  1400/ 2983 batches | lr 1.00 | ms/batch 17.40 | loss  5.30 | ppl   199.76\n","| epoch  18 |  1600/ 2983 batches | lr 1.00 | ms/batch 17.44 | loss  5.36 | ppl   212.40\n","| epoch  18 |  1800/ 2983 batches | lr 1.00 | ms/batch 17.46 | loss  5.27 | ppl   194.93\n","| epoch  18 |  2000/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  5.30 | ppl   201.11\n","| epoch  18 |  2200/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  5.23 | ppl   186.78\n","| epoch  18 |  2400/ 2983 batches | lr 1.00 | ms/batch 17.45 | loss  5.26 | ppl   192.47\n","| epoch  18 |  2600/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  5.29 | ppl   198.54\n","| epoch  18 |  2800/ 2983 batches | lr 1.00 | ms/batch 17.49 | loss  5.24 | ppl   188.72\n","-----------------------------------------------------------------------------------------\n","| end of epoch  18 | time: 54.23s | valid loss  5.31 | valid ppl   203.25\n","-----------------------------------------------------------------------------------------\n","| epoch  19 |   200/ 2983 batches | lr 1.00 | ms/batch 17.56 | loss  5.29 | ppl   199.09\n","| epoch  19 |   400/ 2983 batches | lr 1.00 | ms/batch 17.50 | loss  5.32 | ppl   203.55\n","| epoch  19 |   600/ 2983 batches | lr 1.00 | ms/batch 17.50 | loss  5.18 | ppl   178.24\n","| epoch  19 |   800/ 2983 batches | lr 1.00 | ms/batch 17.50 | loss  5.25 | ppl   189.95\n","| epoch  19 |  1000/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  5.24 | ppl   188.14\n","| epoch  19 |  1200/ 2983 batches | lr 1.00 | ms/batch 17.50 | loss  5.24 | ppl   188.67\n","| epoch  19 |  1400/ 2983 batches | lr 1.00 | ms/batch 17.51 | loss  5.27 | ppl   193.67\n","| epoch  19 |  1600/ 2983 batches | lr 1.00 | ms/batch 17.50 | loss  5.33 | ppl   206.02\n","| epoch  19 |  1800/ 2983 batches | lr 1.00 | ms/batch 17.55 | loss  5.24 | ppl   188.75\n","| epoch  19 |  2000/ 2983 batches | lr 1.00 | ms/batch 17.51 | loss  5.28 | ppl   195.57\n","| epoch  19 |  2200/ 2983 batches | lr 1.00 | ms/batch 17.53 | loss  5.20 | ppl   180.60\n","| epoch  19 |  2400/ 2983 batches | lr 1.00 | ms/batch 17.52 | loss  5.23 | ppl   185.91\n","| epoch  19 |  2600/ 2983 batches | lr 1.00 | ms/batch 17.48 | loss  5.25 | ppl   191.36\n","| epoch  19 |  2800/ 2983 batches | lr 1.00 | ms/batch 17.51 | loss  5.21 | ppl   182.79\n","-----------------------------------------------------------------------------------------\n","| end of epoch  19 | time: 54.35s | valid loss  5.28 | valid ppl   196.70\n","-----------------------------------------------------------------------------------------\n","| epoch  20 |   200/ 2983 batches | lr 1.00 | ms/batch 17.55 | loss  5.26 | ppl   192.63\n","| epoch  20 |   400/ 2983 batches | lr 1.00 | ms/batch 17.46 | loss  5.29 | ppl   197.54\n","| epoch  20 |   600/ 2983 batches | lr 1.00 | ms/batch 17.47 | loss  5.15 | ppl   172.82\n","| epoch  20 |   800/ 2983 batches | lr 1.00 | ms/batch 17.51 | loss  5.21 | ppl   183.56\n","| epoch  20 |  1000/ 2983 batches | lr 1.00 | ms/batch 17.47 | loss  5.21 | ppl   182.21\n","| epoch  20 |  1200/ 2983 batches | lr 1.00 | ms/batch 17.47 | loss  5.21 | ppl   183.09\n","| epoch  20 |  1400/ 2983 batches | lr 1.00 | ms/batch 17.46 | loss  5.24 | ppl   188.01\n","| epoch  20 |  1600/ 2983 batches | lr 1.00 | ms/batch 17.74 | loss  5.30 | ppl   200.09\n","| epoch  20 |  1800/ 2983 batches | lr 1.00 | ms/batch 18.33 | loss  5.21 | ppl   183.36\n","| epoch  20 |  2000/ 2983 batches | lr 1.00 | ms/batch 17.47 | loss  5.24 | ppl   189.26\n","| epoch  20 |  2200/ 2983 batches | lr 1.00 | ms/batch 17.50 | loss  5.17 | ppl   175.27\n","| epoch  20 |  2400/ 2983 batches | lr 1.00 | ms/batch 17.53 | loss  5.19 | ppl   180.35\n","| epoch  20 |  2600/ 2983 batches | lr 1.00 | ms/batch 17.51 | loss  5.23 | ppl   185.89\n","| epoch  20 |  2800/ 2983 batches | lr 1.00 | ms/batch 17.50 | loss  5.18 | ppl   177.14\n","-----------------------------------------------------------------------------------------\n","| end of epoch  20 | time: 54.51s | valid loss  5.28 | valid ppl   196.87\n","-----------------------------------------------------------------------------------------\n","| epoch  21 |   200/ 2983 batches | lr 0.25 | ms/batch 17.57 | loss  5.25 | ppl   190.80\n","| epoch  21 |   400/ 2983 batches | lr 0.25 | ms/batch 17.47 | loss  5.27 | ppl   195.30\n","| epoch  21 |   600/ 2983 batches | lr 0.25 | ms/batch 17.48 | loss  5.14 | ppl   170.42\n","| epoch  21 |   800/ 2983 batches | lr 0.25 | ms/batch 17.50 | loss  5.20 | ppl   180.88\n","| epoch  21 |  1000/ 2983 batches | lr 0.25 | ms/batch 17.46 | loss  5.19 | ppl   178.95\n","| epoch  21 |  1200/ 2983 batches | lr 0.25 | ms/batch 17.46 | loss  5.19 | ppl   179.15\n","| epoch  21 |  1400/ 2983 batches | lr 0.25 | ms/batch 17.47 | loss  5.22 | ppl   184.03\n","| epoch  21 |  1600/ 2983 batches | lr 0.25 | ms/batch 17.46 | loss  5.26 | ppl   192.92\n","| epoch  21 |  1800/ 2983 batches | lr 0.25 | ms/batch 17.45 | loss  5.18 | ppl   176.96\n","| epoch  21 |  2000/ 2983 batches | lr 0.25 | ms/batch 17.48 | loss  5.21 | ppl   183.86\n","| epoch  21 |  2200/ 2983 batches | lr 0.25 | ms/batch 17.48 | loss  5.13 | ppl   168.69\n","| epoch  21 |  2400/ 2983 batches | lr 0.25 | ms/batch 17.49 | loss  5.16 | ppl   173.65\n","| epoch  21 |  2600/ 2983 batches | lr 0.25 | ms/batch 17.46 | loss  5.19 | ppl   179.38\n","| epoch  21 |  2800/ 2983 batches | lr 0.25 | ms/batch 17.47 | loss  5.13 | ppl   169.65\n","-----------------------------------------------------------------------------------------\n","| end of epoch  21 | time: 54.25s | valid loss  5.24 | valid ppl   188.38\n","-----------------------------------------------------------------------------------------\n","| epoch  22 |   200/ 2983 batches | lr 0.25 | ms/batch 17.54 | loss  5.23 | ppl   186.45\n","| epoch  22 |   400/ 2983 batches | lr 0.25 | ms/batch 17.46 | loss  5.26 | ppl   191.72\n","| epoch  22 |   600/ 2983 batches | lr 0.25 | ms/batch 17.45 | loss  5.12 | ppl   167.80\n","| epoch  22 |   800/ 2983 batches | lr 0.25 | ms/batch 17.43 | loss  5.18 | ppl   177.55\n","| epoch  22 |  1000/ 2983 batches | lr 0.25 | ms/batch 17.48 | loss  5.17 | ppl   175.73\n","| epoch  22 |  1200/ 2983 batches | lr 0.25 | ms/batch 17.43 | loss  5.17 | ppl   176.36\n","| epoch  22 |  1400/ 2983 batches | lr 0.25 | ms/batch 17.48 | loss  5.20 | ppl   181.48\n","| epoch  22 |  1600/ 2983 batches | lr 0.25 | ms/batch 17.46 | loss  5.25 | ppl   191.43\n","| epoch  22 |  1800/ 2983 batches | lr 0.25 | ms/batch 17.46 | loss  5.17 | ppl   175.32\n","| epoch  22 |  2000/ 2983 batches | lr 0.25 | ms/batch 17.47 | loss  5.20 | ppl   181.86\n","| epoch  22 |  2200/ 2983 batches | lr 0.25 | ms/batch 17.47 | loss  5.12 | ppl   167.72\n","| epoch  22 |  2400/ 2983 batches | lr 0.25 | ms/batch 17.49 | loss  5.15 | ppl   172.23\n","| epoch  22 |  2600/ 2983 batches | lr 0.25 | ms/batch 17.49 | loss  5.18 | ppl   177.81\n","| epoch  22 |  2800/ 2983 batches | lr 0.25 | ms/batch 17.47 | loss  5.13 | ppl   168.71\n","-----------------------------------------------------------------------------------------\n","| end of epoch  22 | time: 54.25s | valid loss  5.23 | valid ppl   186.74\n","-----------------------------------------------------------------------------------------\n","| epoch  23 |   200/ 2983 batches | lr 0.25 | ms/batch 17.59 | loss  5.22 | ppl   184.60\n","| epoch  23 |   400/ 2983 batches | lr 0.25 | ms/batch 17.47 | loss  5.24 | ppl   189.47\n","| epoch  23 |   600/ 2983 batches | lr 0.25 | ms/batch 17.47 | loss  5.11 | ppl   165.30\n","| epoch  23 |   800/ 2983 batches | lr 0.25 | ms/batch 17.45 | loss  5.17 | ppl   175.67\n","| epoch  23 |  1000/ 2983 batches | lr 0.25 | ms/batch 17.49 | loss  5.16 | ppl   174.42\n","| epoch  23 |  1200/ 2983 batches | lr 0.25 | ms/batch 17.50 | loss  5.16 | ppl   175.03\n","| epoch  23 |  1400/ 2983 batches | lr 0.25 | ms/batch 17.49 | loss  5.19 | ppl   179.83\n","| epoch  23 |  1600/ 2983 batches | lr 0.25 | ms/batch 17.48 | loss  5.24 | ppl   189.49\n","| epoch  23 |  1800/ 2983 batches | lr 0.25 | ms/batch 17.49 | loss  5.16 | ppl   174.33\n","| epoch  23 |  2000/ 2983 batches | lr 0.25 | ms/batch 17.47 | loss  5.19 | ppl   180.26\n","| epoch  23 |  2200/ 2983 batches | lr 0.25 | ms/batch 17.47 | loss  5.11 | ppl   166.39\n","| epoch  23 |  2400/ 2983 batches | lr 0.25 | ms/batch 17.47 | loss  5.14 | ppl   171.01\n","| epoch  23 |  2600/ 2983 batches | lr 0.25 | ms/batch 17.48 | loss  5.17 | ppl   176.79\n","| epoch  23 |  2800/ 2983 batches | lr 0.25 | ms/batch 17.48 | loss  5.12 | ppl   167.87\n","-----------------------------------------------------------------------------------------\n","| end of epoch  23 | time: 54.28s | valid loss  5.23 | valid ppl   186.31\n","-----------------------------------------------------------------------------------------\n","| epoch  24 |   200/ 2983 batches | lr 0.25 | ms/batch 17.58 | loss  5.21 | ppl   182.28\n","| epoch  24 |   400/ 2983 batches | lr 0.25 | ms/batch 17.52 | loss  5.23 | ppl   187.39\n","| epoch  24 |   600/ 2983 batches | lr 0.25 | ms/batch 17.50 | loss  5.10 | ppl   164.17\n","| epoch  24 |   800/ 2983 batches | lr 0.25 | ms/batch 17.51 | loss  5.16 | ppl   173.95\n","| epoch  24 |  1000/ 2983 batches | lr 0.25 | ms/batch 18.04 | loss  5.15 | ppl   172.94\n","| epoch  24 |  1200/ 2983 batches | lr 0.25 | ms/batch 17.83 | loss  5.16 | ppl   174.00\n","| epoch  24 |  1400/ 2983 batches | lr 0.25 | ms/batch 17.51 | loss  5.18 | ppl   178.06\n","| epoch  24 |  1600/ 2983 batches | lr 0.25 | ms/batch 17.52 | loss  5.23 | ppl   187.49\n","| epoch  24 |  1800/ 2983 batches | lr 0.25 | ms/batch 17.56 | loss  5.15 | ppl   172.33\n","| epoch  24 |  2000/ 2983 batches | lr 0.25 | ms/batch 17.55 | loss  5.19 | ppl   178.91\n","| epoch  24 |  2200/ 2983 batches | lr 0.25 | ms/batch 17.55 | loss  5.11 | ppl   165.41\n","| epoch  24 |  2400/ 2983 batches | lr 0.25 | ms/batch 17.53 | loss  5.13 | ppl   169.72\n","| epoch  24 |  2600/ 2983 batches | lr 0.25 | ms/batch 17.48 | loss  5.17 | ppl   175.16\n","| epoch  24 |  2800/ 2983 batches | lr 0.25 | ms/batch 17.50 | loss  5.12 | ppl   166.71\n","-----------------------------------------------------------------------------------------\n","| end of epoch  24 | time: 54.57s | valid loss  5.22 | valid ppl   185.63\n","-----------------------------------------------------------------------------------------\n","| epoch  25 |   200/ 2983 batches | lr 0.25 | ms/batch 17.61 | loss  5.20 | ppl   180.62\n","| epoch  25 |   400/ 2983 batches | lr 0.25 | ms/batch 17.48 | loss  5.22 | ppl   185.79\n","| epoch  25 |   600/ 2983 batches | lr 0.25 | ms/batch 17.51 | loss  5.09 | ppl   162.70\n","| epoch  25 |   800/ 2983 batches | lr 0.25 | ms/batch 17.50 | loss  5.15 | ppl   172.88\n","| epoch  25 |  1000/ 2983 batches | lr 0.25 | ms/batch 17.48 | loss  5.14 | ppl   171.26\n","| epoch  25 |  1200/ 2983 batches | lr 0.25 | ms/batch 17.52 | loss  5.15 | ppl   172.24\n","| epoch  25 |  1400/ 2983 batches | lr 0.25 | ms/batch 17.48 | loss  5.18 | ppl   177.03\n","| epoch  25 |  1600/ 2983 batches | lr 0.25 | ms/batch 17.53 | loss  5.23 | ppl   186.87\n","| epoch  25 |  1800/ 2983 batches | lr 0.25 | ms/batch 17.53 | loss  5.14 | ppl   171.31\n","| epoch  25 |  2000/ 2983 batches | lr 0.25 | ms/batch 17.52 | loss  5.18 | ppl   178.36\n","| epoch  25 |  2200/ 2983 batches | lr 0.25 | ms/batch 17.51 | loss  5.10 | ppl   164.25\n","| epoch  25 |  2400/ 2983 batches | lr 0.25 | ms/batch 17.51 | loss  5.13 | ppl   168.86\n","| epoch  25 |  2600/ 2983 batches | lr 0.25 | ms/batch 17.47 | loss  5.16 | ppl   174.47\n","| epoch  25 |  2800/ 2983 batches | lr 0.25 | ms/batch 17.48 | loss  5.11 | ppl   165.90\n","-----------------------------------------------------------------------------------------\n","| end of epoch  25 | time: 54.34s | valid loss  5.22 | valid ppl   184.38\n","-----------------------------------------------------------------------------------------\n","| epoch  26 |   200/ 2983 batches | lr 0.25 | ms/batch 17.56 | loss  5.19 | ppl   179.12\n","| epoch  26 |   400/ 2983 batches | lr 0.25 | ms/batch 17.48 | loss  5.22 | ppl   184.59\n","| epoch  26 |   600/ 2983 batches | lr 0.25 | ms/batch 17.49 | loss  5.08 | ppl   161.49\n","| epoch  26 |   800/ 2983 batches | lr 0.25 | ms/batch 17.46 | loss  5.14 | ppl   170.85\n","| epoch  26 |  1000/ 2983 batches | lr 0.25 | ms/batch 17.45 | loss  5.14 | ppl   170.27\n","| epoch  26 |  1200/ 2983 batches | lr 0.25 | ms/batch 17.46 | loss  5.14 | ppl   171.41\n","| epoch  26 |  1400/ 2983 batches | lr 0.25 | ms/batch 17.46 | loss  5.17 | ppl   175.88\n","| epoch  26 |  1600/ 2983 batches | lr 0.25 | ms/batch 17.48 | loss  5.22 | ppl   185.85\n","| epoch  26 |  1800/ 2983 batches | lr 0.25 | ms/batch 17.45 | loss  5.14 | ppl   170.12\n","| epoch  26 |  2000/ 2983 batches | lr 0.25 | ms/batch 17.46 | loss  5.18 | ppl   176.89\n","| epoch  26 |  2200/ 2983 batches | lr 0.25 | ms/batch 17.44 | loss  5.09 | ppl   162.69\n","| epoch  26 |  2400/ 2983 batches | lr 0.25 | ms/batch 17.45 | loss  5.12 | ppl   167.17\n","| epoch  26 |  2600/ 2983 batches | lr 0.25 | ms/batch 17.47 | loss  5.16 | ppl   173.37\n","| epoch  26 |  2800/ 2983 batches | lr 0.25 | ms/batch 17.46 | loss  5.10 | ppl   164.60\n","-----------------------------------------------------------------------------------------\n","| end of epoch  26 | time: 54.23s | valid loss  5.21 | valid ppl   183.43\n","-----------------------------------------------------------------------------------------\n","| epoch  27 |   200/ 2983 batches | lr 0.25 | ms/batch 17.54 | loss  5.18 | ppl   178.10\n","| epoch  27 |   400/ 2983 batches | lr 0.25 | ms/batch 17.48 | loss  5.21 | ppl   183.00\n","| epoch  27 |   600/ 2983 batches | lr 0.25 | ms/batch 17.49 | loss  5.08 | ppl   160.12\n","| epoch  27 |   800/ 2983 batches | lr 0.25 | ms/batch 17.48 | loss  5.13 | ppl   169.81\n","| epoch  27 |  1000/ 2983 batches | lr 0.25 | ms/batch 17.44 | loss  5.13 | ppl   168.93\n","| epoch  27 |  1200/ 2983 batches | lr 0.25 | ms/batch 17.51 | loss  5.13 | ppl   169.60\n","| epoch  27 |  1400/ 2983 batches | lr 0.25 | ms/batch 17.46 | loss  5.16 | ppl   174.36\n","| epoch  27 |  1600/ 2983 batches | lr 0.25 | ms/batch 17.45 | loss  5.21 | ppl   183.85\n","-----------------------------------------------------------------------------------------\n","Exiting from training early\n"]}]},{"cell_type":"markdown","source":["## Load the best saved model."],"metadata":{"id":"m01UK5qrQqNL"}},{"cell_type":"code","source":["\n","with open(saved_model, 'rb') as f:\n","    model = torch.load(f)\n","    # after load the rnn params are not a continuous chunk of memory\n","    # this makes them a continuous chunk, and will speed up forward pass\n","    # Currently, only rnn model supports flatten_parameters function.\n","    if model_name in ['RNN_TANH', 'RNN_RELU', 'LSTM', 'GRU']:\n","        model.rnn.flatten_parameters()\n"],"metadata":{"id":"0YfOSNMaPSey","executionInfo":{"status":"ok","timestamp":1667608826210,"user_tz":240,"elapsed":5,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["## Run on test data."],"metadata":{"id":"uvgEpxL9XH52"}},{"cell_type":"code","source":["\n","test_loss = evaluate(test_data)\n","print('=' * 89)\n","print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n","    test_loss, math.exp(test_loss)))\n","print('=' * 89)\n","\n","if len(onnx_export) > 0:\n","    # Export the model in ONNX format.\n","    export_onnx(onnx_export, batch_size=1, seq_len=bptt)"],"metadata":{"id":"oIcIMQk6XG5W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667610579678,"user_tz":240,"elapsed":2957,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}},"outputId":"9cc4b9e5-093a-48ae-c2c2-4fea066b25c4"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["=========================================================================================\n","| End of training | test loss  5.11 | test ppl   166.31\n","=========================================================================================\n"]}]},{"cell_type":"markdown","source":["#Generate Text"],"metadata":{"id":"g4hILwO-YVmG"}},{"cell_type":"code","source":["# Model parameters.\n","data_loc='./' #help='location of the data corpus'\n","checkpoint='./model.pt' #help='model checkpoint to use'\n","outf='generated.txt' #help='output file for generated text')\n","words=10 #help='number of words to generate')\n","seed=1111 #help='random seed')\n","cuda= True #help='use CUDA')\n","mps=False #help='enables macOS GPU training')\n","temperature=0.1 #help='temperature - higher will increase diversity')\n","log_interval=100 #help='reporting interval')"],"metadata":{"id":"WP4T8fwhYe57","executionInfo":{"status":"ok","timestamp":1667610617841,"user_tz":240,"elapsed":1,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Set the random seed manually for reproducibility.\n","torch.manual_seed(seed)\n","if torch.cuda.is_available():\n","    if not cuda:\n","        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda.\")\n","if torch.backends.mps.is_available():\n","    if not mps:\n","        print(\"WARNING: You have mps device, to enable macOS GPU run with --mps.\")\n","        \n","use_mps = mps and torch.backends.mps.is_available()\n","if cuda:\n","    device = torch.device(\"cuda\")\n","elif use_mps:\n","    device = torch.device(\"mps\")\n","else:\n","    device = torch.device(\"cpu\")\n","\n","if temperature < 1e-3:\n","    parser.error(\"--temperature has to be greater or equal 1e-3.\")\n"],"metadata":{"id":"Tj3q_LyKZuLB","executionInfo":{"status":"ok","timestamp":1667610620710,"user_tz":240,"elapsed":1,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n","print('input before: ', input)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xfc-LhLVWFR0","executionInfo":{"status":"ok","timestamp":1667610623659,"user_tz":240,"elapsed":350,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}},"outputId":"275b977b-04f2-4312-8340-8b4b775fdf0d"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["input before:  tensor([[18564]], device='cuda:0')\n"]}]},{"cell_type":"code","source":["len(input[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f5BICJ4PhYrI","executionInfo":{"status":"ok","timestamp":1667613738650,"user_tz":240,"elapsed":391,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}},"outputId":"79ef070e-a01f-4c7d-874e-4591829ad2a2"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["txts=\"\"\n","with open(checkpoint, 'rb') as f:\n","    model = torch.load(f, map_location=device)\n","model.eval()\n","\n","corpus = Corpus(data_loc)\n","ntokens = len(corpus.dictionary)\n","\n","is_transformer_model = hasattr(model, 'model_type') and model.model_type == 'Transformer'\n","input = corpus.tokenize_single(\"birth date is\") \n","input=input.to(device)  #torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n","if not is_transformer_model:\n","    hidden = model.init_hidden(len(input[0]))\n","\n","print('input before: ', input)\n","#with open(outf, 'w') as outf:\n","with torch.no_grad():  # no tracking history\n","    for i in range(words):\n","        if is_transformer_model:\n","            output = model(input, False)\n","            word_weights = output[-1].squeeze().div(temperature).exp().cpu()\n","            word_idx = torch.multinomial(word_weights, 1)[0]\n","            word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n","            input = torch.cat([input, word_tensor], 0)\n","        else:\n","            output, hidden = model(input, hidden)\n","            print('output:', output)\n","            word_weights = output.squeeze().div(temperature).exp().cpu()\n","            #print('word_weights:', word_weights)\n","            word_idx = torch.multinomial(word_weights, 1)[0]\n","            #print('word_idx:', word_idx)\n","            #input.fill_(word_idx)\n","            print('input after: ', input)\n","\n","        word = corpus.dictionary.idx2word[word_idx]\n","        print('word:',word)\n","        txts= txts+' '+ word   #outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n","\n","        if i % log_interval == 0:\n","            print('| Generated {}/{} words'.format(i, words))"],"metadata":{"id":"XEVqwyeBaZEA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667613907515,"user_tz":240,"elapsed":2746,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}},"outputId":"8b049ca2-c020-4800-fee7-03f74341ea95"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["input before:  tensor([[963, 962,  26,   0]], device='cuda:0')\n","output: tensor([[ -5.7322,  -8.5891, -10.0856,  ..., -10.8387, -11.0590, -11.0249],\n","        [ -5.1870,  -8.2396,  -9.9498,  ..., -10.7094, -11.0175, -11.0859],\n","        [ -8.2082, -11.7987, -10.6106,  ..., -10.5849, -10.8902, -10.9479],\n","        [ -5.4247,  -7.7961,  -9.6939,  ..., -10.4860, -10.4168, -10.5533]],\n","       device='cuda:0')\n","input after:  tensor([[963, 962,  26,   0]], device='cuda:0')\n","word: ,\n","| Generated 0/10 words\n","output: tensor([[ -5.3058,  -8.2991, -10.8480,  ..., -12.4206, -12.6743, -12.4950],\n","        [ -4.3283,  -7.6440, -11.0347,  ..., -12.6723, -13.1544, -13.1937],\n","        [ -9.2419, -13.6867, -11.4608,  ..., -11.2025, -11.6599, -11.9416],\n","        [ -5.6126,  -7.4718,  -9.3702,  ..., -10.9136, -10.4379, -10.8456]],\n","       device='cuda:0')\n","input after:  tensor([[963, 962,  26,   0]], device='cuda:0')\n","word: ,\n","output: tensor([[ -5.9430,  -8.9350, -11.9283,  ..., -14.4407, -14.6003, -14.3024],\n","        [ -5.6200,  -8.7351, -13.1187,  ..., -15.5595, -16.0235, -15.8829],\n","        [ -9.9201, -14.3675, -11.9782,  ..., -11.8763, -12.3588, -12.8783],\n","        [ -5.1443,  -5.9794,  -9.1839,  ..., -11.2507, -10.5965, -11.1130]],\n","       device='cuda:0')\n","input after:  tensor([[963, 962,  26,   0]], device='cuda:0')\n","word: .\n","output: tensor([[ -6.7520,  -9.3003, -12.0858,  ..., -15.2462, -15.3295, -15.0447],\n","        [ -7.0195,  -9.8825, -13.8776,  ..., -16.7236, -17.1395, -17.0001],\n","        [ -9.8461, -14.5101, -12.0738,  ..., -12.2664, -12.7561, -13.3350],\n","        [ -4.6713,  -3.9889,  -9.2351,  ..., -11.5489, -10.8479, -11.3398]],\n","       device='cuda:0')\n","input after:  tensor([[963, 962,  26,   0]], device='cuda:0')\n","word: .\n","output: tensor([[ -7.1924,  -9.1769, -11.9502,  ..., -15.4850, -15.4676, -15.1808],\n","        [ -7.8721, -10.9251, -14.1470,  ..., -17.0779, -17.4919, -17.3924],\n","        [ -9.6429, -14.5403, -12.0771,  ..., -12.4424, -12.9226, -13.5203],\n","        [ -4.5925,  -2.5304,  -9.4885,  ..., -11.8169, -11.1137, -11.5161]],\n","       device='cuda:0')\n","input after:  tensor([[963, 962,  26,   0]], device='cuda:0')\n","word: .\n","output: tensor([[ -7.2253,  -8.8582, -11.7662,  ..., -15.5508, -15.4530, -15.0985],\n","        [ -8.4032, -11.8014, -14.3431,  ..., -17.2784, -17.6933, -17.5944],\n","        [ -9.5552, -14.5439, -12.0759,  ..., -12.5213, -12.9648, -13.5907],\n","        [ -4.6001,  -2.5332,  -9.7494,  ..., -11.8965, -11.2599, -11.5610]],\n","       device='cuda:0')\n","input after:  tensor([[963, 962,  26,   0]], device='cuda:0')\n","word: .\n","output: tensor([[ -7.0484,  -8.6055, -11.5424,  ..., -15.5447, -15.3726, -14.9328],\n","        [ -8.7490, -12.3560, -14.4529,  ..., -17.3708, -17.8097, -17.6827],\n","        [ -9.5118, -14.5152, -12.0718,  ..., -12.5643, -12.9656, -13.6124],\n","        [ -4.4801,  -3.4266,  -9.8782,  ..., -11.8839, -11.3594, -11.6034]],\n","       device='cuda:0')\n","input after:  tensor([[963, 962,  26,   0]], device='cuda:0')\n","word: .\n","output: tensor([[ -6.7909,  -8.4724, -11.3449,  ..., -15.5396, -15.2651, -14.7715],\n","        [ -8.9362, -12.6331, -14.5150,  ..., -17.4039, -17.8838, -17.7223],\n","        [ -9.4768, -14.4745, -12.0692,  ..., -12.5899, -12.9592, -13.6169],\n","        [ -4.2568,  -3.3950,  -9.9338,  ..., -11.9578, -11.5009, -11.7266]],\n","       device='cuda:0')\n","input after:  tensor([[963, 962,  26,   0]], device='cuda:0')\n","word: .\n","output: tensor([[ -6.5182,  -8.4136, -11.2098,  ..., -15.5644, -15.1766, -14.6525],\n","        [ -9.0330, -12.7907, -14.5588,  ..., -17.4199, -17.9438, -17.7467],\n","        [ -9.4468, -14.4345, -12.0692,  ..., -12.6036, -12.9537, -13.6159],\n","        [ -4.1752,  -2.2164, -10.0189,  ..., -12.1090, -11.6568, -11.8559]],\n","       device='cuda:0')\n","input after:  tensor([[963, 962,  26,   0]], device='cuda:0')\n","word: .\n","output: tensor([[ -6.2712,  -8.3851, -11.1299,  ..., -15.6075, -15.1190, -14.5686],\n","        [ -9.0678, -12.8873, -14.5726,  ..., -17.4172, -17.9808, -17.7521],\n","        [ -9.4240, -14.3975, -12.0708,  ..., -12.6085, -12.9487, -13.6120],\n","        [ -4.3103,  -1.7476, -10.1613,  ..., -12.1881, -11.7357, -11.8864]],\n","       device='cuda:0')\n","input after:  tensor([[963, 962,  26,   0]], device='cuda:0')\n","word: .\n"]}]},{"cell_type":"code","source":["txts"],"metadata":{"id":"ANNUnwk1iU49","executionInfo":{"status":"ok","timestamp":1667613941130,"user_tz":240,"elapsed":518,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}},"outputId":"2abfb42d-3370-4af0-9524-b4bb8eda6ae2","colab":{"base_uri":"https://localhost:8080/","height":36}},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' , , . . . . . . . .'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["def generate_txt(inp):\n","  propmpt1=[\"birthday\", \"birthdate\"]\n","  prompt2=[\"born on\", \"born in\"]\n","  "],"metadata":{"id":"QQrCCkM-auvx","executionInfo":{"status":"ok","timestamp":1667610439935,"user_tz":240,"elapsed":6,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}}},"execution_count":11,"outputs":[]}]}